{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90e5464a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'국내 전반적인 경기침체로 상가 건물주의 수익도 전국적인 감소세를 보이고 있는 것으로 나타났다. 수익형 부동산 연구개발기업 상가정보연구소는 한국감정원 통계를 분석한 결과 전국 중대형 상가 순영업소득(부동산에서 발생하는 임대수입, 기타수입에서 제반 경비를 공제한 순소득)이 1분기 ㎡당 3만4200원에서 3분기 2만5800원으로 감소했다고 17일 밝혔다. 수도권, 세종시, 지방광역시에서 순영업소득이 가장 많이 감소한 지역은 3분기 1만3100원을 기록한 울산으로, 1분기 1만9100원 대비 31.4% 감소했다. 이어 대구(-27.7%), 서울(-26.9%), 광주(-24.9%), 부산(-23.5%), 세종(-23.4%), 대전(-21%), 경기(-19.2%), 인천(-18.5%) 순으로 감소했다. 지방 도시의 경우도 비슷했다. 경남의 3분기 순영업소득은 1만2800원으로 1분기 1만7400원 대비 26.4% 감소했으며 제주(-25.1%), 경북(-24.1%), 충남(-20.9%), 강원(-20.9%), 전남(-20.1%), 전북(-17%), 충북(-15.3%) 등도 감소세를 보였다. 조현택 상가정보연구소 연구원은 \"올해 내수 경기의 침체된 분위기가 유지되며 상가, 오피스 등을 비롯한 수익형 부동산 시장의 분위기도 경직된 모습을 보였고 오피스텔, 지식산업센터 등의 수익형 부동산 공급도 증가해 공실의 위험도 늘었다\"며 \"실제 올 3분기 전국 중대형 상가 공실률은 11.5%를 기록하며 1분기 11.3% 대비 0.2% 포인트 증가했다\"고 말했다. 그는 \"최근 소셜커머스(SNS를 통한 전자상거래), 음식 배달 중개 애플리케이션, 중고 물품 거래 애플리케이션 등의 사용 증가로 오프라인 매장에 영향을 미쳤다\"며 \"향후 지역, 콘텐츠에 따른 상권 양극화 현상은 심화될 것으로 보인다\"고 덧붙였다.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = '국내 전반적인 경기침체로 상가 건물주의 수익도 전국적인 감소세를 보이고 있는 것으로 나타났다. 수익형 부동산 연구개발기업 상가정보연구소는 한국감정원 통계를 분석한 결과 전국 중대형 상가 순영업소득(부동산에서 발생하는 임대수입, 기타수입에서 제반 경비를 공제한 순소득)이 1분기 ㎡당 3만4200원에서 3분기 2만5800원으로 감소했다고 17일 밝혔다. 수도권, 세종시, 지방광역시에서 순영업소득이 가장 많이 감소한 지역은 3분기 1만3100원을 기록한 울산으로, 1분기 1만9100원 대비 31.4% 감소했다. 이어 대구(-27.7%), 서울(-26.9%), 광주(-24.9%), 부산(-23.5%), 세종(-23.4%), 대전(-21%), 경기(-19.2%), 인천(-18.5%) 순으로 감소했다. 지방 도시의 경우도 비슷했다. 경남의 3분기 순영업소득은 1만2800원으로 1분기 1만7400원 대비 26.4% 감소했으며 제주(-25.1%), 경북(-24.1%), 충남(-20.9%), 강원(-20.9%), 전남(-20.1%), 전북(-17%), 충북(-15.3%) 등도 감소세를 보였다. 조현택 상가정보연구소 연구원은 \"올해 내수 경기의 침체된 분위기가 유지되며 상가, 오피스 등을 비롯한 수익형 부동산 시장의 분위기도 경직된 모습을 보였고 오피스텔, 지식산업센터 등의 수익형 부동산 공급도 증가해 공실의 위험도 늘었다\"며 \"실제 올 3분기 전국 중대형 상가 공실률은 11.5%를 기록하며 1분기 11.3% 대비 0.2% 포인트 증가했다\"고 말했다. 그는 \"최근 소셜커머스(SNS를 통한 전자상거래), 음식 배달 중개 애플리케이션, 중고 물품 거래 애플리케이션 등의 사용 증가로 오프라인 매장에 영향을 미쳤다\"며 \"향후 지역, 콘텐츠에 따른 상권 양극화 현상은 심화될 것으로 보인다\"고 덧붙였다.'\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed513f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 424, 768])\n",
      "tensor([[ 0.3131,  0.1495, -0.2805,  ..., -1.0458,  0.2955,  0.2060],\n",
      "        [-0.6040, -0.3774,  0.7601,  ..., -0.2936,  0.6367, -0.2034],\n",
      "        [-0.9364, -0.4989,  1.5890,  ..., -0.1287,  0.1404,  0.0332],\n",
      "        ...,\n",
      "        [-0.7169,  0.1413,  1.1147,  ..., -1.0553,  0.1065, -0.3283],\n",
      "        [ 0.0473, -0.1965,  0.2112,  ..., -1.1475, -0.5695,  0.0044],\n",
      "        [ 0.2168,  0.0173, -0.6936,  ..., -0.9997, -0.1208,  0.1229]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BartForConditionalGeneration, BertModel, BertConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "input_ids = tokenizer_bert.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model_bert(input_ids)[0] # Models outputs are now tuples\n",
    "print(last_hidden_states.shape)\n",
    "print(last_hidden_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "199d8d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1831e-01, -5.8520e-01,  2.8208e-01,  1.3134e-01, -5.3637e-01,\n",
      "          6.4086e-02, -5.1276e-01, -7.0598e-01, -2.5589e-01, -2.9783e-01,\n",
      "          1.0050e+00,  6.5010e-01, -3.2034e-01, -7.5098e-01, -6.5400e-01,\n",
      "          1.5607e-01, -6.3496e-02, -4.9798e-01,  4.2883e-01,  4.3103e-01,\n",
      "         -4.3904e-01, -6.6510e-01, -1.9600e-01,  1.4433e-01, -1.8935e-01,\n",
      "          6.0381e-01, -8.4180e-01, -5.7096e-01, -4.5713e-02,  6.6888e-02,\n",
      "          1.1719e+00,  2.8450e-01,  1.8008e-01,  8.2213e-01, -4.6427e-02,\n",
      "          7.3754e-01, -1.2670e+00, -8.3744e-01,  1.9948e-01, -7.6356e-01,\n",
      "         -8.1205e-02,  6.2777e-01,  7.7750e-01,  3.5418e-02, -5.1461e-01,\n",
      "         -5.3380e-01, -3.3792e-02,  4.0549e-01, -1.2518e+00,  3.1446e-01,\n",
      "         -1.0836e+00, -3.1691e-01,  5.0189e-02, -3.3321e-01, -5.1094e-01,\n",
      "         -1.9821e-02, -6.3643e-01, -3.4611e-01,  4.1599e-01,  4.0669e-01,\n",
      "         -5.9192e-01, -6.5608e-02, -4.9086e-01,  1.2308e-01, -6.3949e-01,\n",
      "          3.4707e-01,  6.1347e-03, -1.6273e-01,  6.0142e-03,  1.3774e-02,\n",
      "          6.5955e-02, -5.8254e-01, -1.8975e-01, -3.1025e-01,  1.0753e-01,\n",
      "          1.6696e-01,  4.8571e-04,  8.1319e-01, -9.2266e-01,  2.8369e-01,\n",
      "         -3.3402e-01, -2.5585e-01, -4.9024e-02,  5.0061e-01,  2.7692e-01,\n",
      "         -6.8190e-01, -3.7913e-01, -6.9151e-02, -1.1559e-01, -5.3632e-01,\n",
      "         -2.3040e-01, -1.1306e+00,  5.1643e-01,  1.7139e-01,  1.0905e-01,\n",
      "          1.7092e-01, -3.9132e-03, -1.1693e-01, -5.6758e-01,  2.3725e-01,\n",
      "         -5.7965e-01, -1.1297e+00, -1.9186e-01, -2.6502e-01,  2.6036e-01,\n",
      "          1.7745e-01,  2.7394e-01,  3.3648e-02, -2.8506e-01, -2.5175e-01,\n",
      "         -8.9075e-02,  3.0540e-01, -3.0908e-01,  7.4178e-01, -1.1497e+00,\n",
      "          3.6436e-01, -3.9458e-01, -3.1906e-01, -1.8064e-01, -7.7973e-01,\n",
      "         -2.1696e-01,  6.8329e-02, -2.0657e-01, -8.5253e-01, -5.7023e-01,\n",
      "         -8.2793e-03,  4.0596e-01, -7.8779e-01,  7.7675e-01, -4.1804e-01,\n",
      "          3.4862e-01, -3.6812e-01,  1.3069e-01, -8.8943e-01, -2.9611e-01,\n",
      "          2.4600e-01,  1.7834e-01, -8.0127e-01, -4.6572e-01,  5.1560e-01,\n",
      "         -1.4278e-02,  5.6967e-01, -2.1604e-01, -3.4536e-01, -2.0124e-01,\n",
      "          4.0942e-02, -1.5859e-02,  2.4294e-02, -6.0945e-01, -8.1639e-01,\n",
      "          3.1181e-01, -7.4642e-01, -4.1041e-01,  1.8117e-01,  5.9385e-01,\n",
      "          2.5940e-01, -5.3181e-01,  4.8887e-01,  4.5634e-01,  2.9001e-01,\n",
      "         -4.3341e-01,  4.6548e-01, -3.7135e-01, -5.7689e-01, -5.5299e-01,\n",
      "          2.6038e-01, -4.5094e-01, -1.7256e-02,  1.9306e-01, -2.5268e-01,\n",
      "          7.2867e-01,  2.1822e-01, -5.2311e-01,  3.9638e-01, -4.6981e-02,\n",
      "         -4.2522e-01,  1.5439e-01,  8.8114e-01,  4.7249e-01,  3.6877e-01,\n",
      "         -1.9879e-01, -5.3099e-01,  2.6141e-01,  8.5325e-02, -1.5393e+00,\n",
      "         -1.0892e-01,  3.0875e-01,  7.1786e-01,  1.4638e-01, -8.0518e-02,\n",
      "         -3.8472e-01, -8.9116e-01,  7.6945e-01, -7.1984e-01, -8.5192e-01,\n",
      "         -1.4139e+00,  1.2411e-01, -6.5646e-01, -3.1371e-01, -3.1741e-01,\n",
      "         -7.3566e-01, -2.0292e-01, -1.0150e-01, -6.1203e-01, -4.8388e-01,\n",
      "          1.8494e-01,  1.1478e-01, -3.5070e-01, -6.9473e-01,  6.7064e-02,\n",
      "         -1.2849e-02, -2.5911e-01, -5.1703e-01, -4.1900e-01,  7.1568e-01,\n",
      "          6.9685e-01, -3.2233e-01, -5.5535e-01,  1.5296e-01, -5.0404e-01,\n",
      "          1.4568e-01,  2.8165e-01,  2.0252e-02,  1.6116e-01, -3.9847e-01,\n",
      "          6.3785e-01, -5.5769e-01,  1.2614e+00,  1.4872e-01, -1.7856e-01,\n",
      "         -3.9906e-01, -2.2387e-02, -5.7623e-02, -6.0629e-01, -1.0025e+00,\n",
      "         -6.3923e-01,  8.2142e-01,  2.7157e-01,  2.9605e-01,  4.1697e-02,\n",
      "          5.1438e-01, -2.6446e-01,  8.4339e-02, -4.0718e-01, -1.5506e-01,\n",
      "         -8.3752e-01,  4.6747e-01,  6.4409e-01,  4.6532e-01,  1.8040e-01,\n",
      "         -2.0245e-01, -3.6138e-01, -1.0214e+00,  8.3629e-01,  1.4268e-01,\n",
      "         -3.4104e-01, -3.1830e-01, -2.4825e-01,  4.1736e-01,  6.8924e-02,\n",
      "         -3.8351e-01, -2.0524e+00,  4.0581e-01, -6.2973e-02, -1.7235e-01,\n",
      "          2.0740e-01, -2.9094e-01,  6.8089e-01,  1.9868e-01, -4.1674e-01,\n",
      "          9.0638e-01, -2.0201e-01, -5.8878e-02,  6.0668e-01,  5.9236e-01,\n",
      "         -3.4984e-01, -1.0959e+00, -1.3380e+00, -5.0889e-01,  1.7549e-01,\n",
      "          1.7820e-01, -3.4240e-01,  4.1173e-02,  5.3480e-01,  1.5223e-01,\n",
      "          1.7410e-01, -4.1938e-01,  4.2326e-01, -3.6620e-01, -2.1323e-01,\n",
      "         -9.3862e-01,  6.2118e-01,  3.1691e-01,  9.0437e-02,  1.8303e-01,\n",
      "         -4.6222e-01,  8.1109e-02,  2.2775e-01, -6.7759e-01, -1.8173e-01,\n",
      "         -2.3052e-01, -7.6705e-01, -8.3333e-02, -1.8126e-01, -1.1552e+00,\n",
      "         -1.6637e-01, -2.0254e-01, -1.5445e-01,  5.9065e-01,  7.2013e-02,\n",
      "         -2.8097e-01, -9.4239e-01, -8.7535e-03,  6.9122e-02, -2.5359e-01,\n",
      "          1.0059e-01,  1.3224e-01, -2.9553e-01,  1.0482e+00, -4.8385e-01,\n",
      "          2.9854e-01,  8.6998e-01,  4.2845e-02, -9.8321e-02,  2.7141e-01,\n",
      "         -1.1198e-01, -2.3962e-01, -2.2341e-01, -4.8548e-01, -5.4230e-01,\n",
      "          1.0779e-01, -4.4959e-01,  1.1217e-02,  3.9398e-01, -3.3729e-02,\n",
      "         -1.7190e-01, -1.2183e-01,  1.1669e-01,  6.6342e-01, -1.2497e-01,\n",
      "          2.2636e-01,  5.2287e-01,  7.1758e-01, -4.8587e-01, -7.1716e-02,\n",
      "         -5.8291e-01, -8.4315e-01,  1.1406e-01,  4.2016e-01, -2.9547e-01,\n",
      "         -2.3464e-01, -1.4132e+00,  1.0504e-01, -2.3284e-01, -9.9812e-02,\n",
      "          2.8735e-01,  5.8649e-01,  7.0694e-01, -2.3751e-01,  6.7413e-02,\n",
      "         -1.0930e+00,  1.8957e-03,  5.8013e-01,  5.2546e-01,  6.4805e-01,\n",
      "         -1.6928e-01,  1.2221e-01,  5.1374e-01, -1.4562e-01,  6.7396e-02,\n",
      "         -4.5093e-01, -7.3232e-02, -1.2941e-01, -4.6860e-01, -1.1841e+00,\n",
      "          8.1340e-01, -4.1367e-01,  7.2913e-01, -2.0026e-01,  8.2974e-02,\n",
      "          1.1751e-01, -3.4636e-01,  1.1385e-01, -2.1906e-01, -1.9978e-01,\n",
      "         -4.0902e-01,  1.8637e-01, -4.0551e-01,  2.0559e-01, -8.7818e-01,\n",
      "         -2.5420e-01,  8.4338e-01, -1.2411e-01,  8.6922e-03,  1.9036e-01,\n",
      "         -8.9262e-02, -2.5065e-02,  7.1257e-01,  1.2236e-02,  4.3582e-01,\n",
      "         -9.9800e-02,  4.7683e-02,  1.4064e-01, -5.4849e-02, -2.8879e-01,\n",
      "         -3.6917e-01,  3.0068e-02,  6.0258e-01, -4.1874e-01,  1.8837e-01,\n",
      "          2.1301e-01, -3.4458e-01,  2.7004e-01,  1.3463e-01,  7.0806e-01,\n",
      "          2.6290e-01,  3.7632e-01,  6.8121e-01,  5.7668e-01,  2.3430e-01,\n",
      "          7.8282e-03, -9.4068e-01, -4.8122e-02,  9.2565e-01, -1.0661e-01,\n",
      "          1.0329e-01,  1.1460e-01, -6.2739e-02,  8.7819e-02, -3.0879e-01,\n",
      "          3.5954e-01,  3.7689e-01, -3.7123e-01,  5.3617e-01, -2.0421e-01,\n",
      "         -1.5918e-01,  1.9308e-01,  8.5556e-01,  4.3984e-01, -3.2679e-01,\n",
      "          3.7081e-01, -8.8417e-02, -1.6969e-01, -4.9669e-02, -5.7355e-01,\n",
      "         -1.6002e-01, -8.0413e-01, -7.0068e-01, -2.8235e-02, -7.7888e-01,\n",
      "          4.2947e-01,  7.6041e-02,  1.5136e-01, -5.2887e-01, -1.9962e-01,\n",
      "         -2.8561e-01, -3.3279e-02, -1.1477e+00,  6.1611e-02, -1.3354e-01,\n",
      "         -1.9073e-01, -9.6132e-02, -6.0471e-02, -8.0865e-01,  4.2603e-01,\n",
      "          3.1652e-01, -7.2104e-01,  9.8330e-02, -5.0329e-02, -6.2089e-01,\n",
      "         -3.9399e-01,  1.1850e-01, -6.4572e-01, -3.2150e-01, -9.0176e-03,\n",
      "         -2.4172e-01,  4.4210e-01, -1.9338e-01, -5.1098e-01,  3.1406e-01,\n",
      "         -1.1642e-01,  3.4668e-01,  2.2970e-01, -1.8694e-01, -2.9307e-01,\n",
      "         -3.1291e-01,  2.8456e-01,  3.8510e-01,  1.9632e-01, -4.1796e-01,\n",
      "          1.6871e-01,  1.1222e-01,  5.7659e-02, -5.3368e-01, -1.0747e+00,\n",
      "         -9.8244e-02,  3.7927e+00,  4.9895e-01,  9.3875e-02,  7.2431e-01,\n",
      "         -1.3923e-01, -5.8207e-01,  7.0291e-02, -3.1841e-01, -1.6663e-01,\n",
      "          3.3019e-02,  5.4997e-01, -5.0991e-01, -6.0839e-01,  1.1599e-01,\n",
      "         -6.6903e-01, -8.2033e-02,  1.5705e-01,  6.4711e-01, -7.3366e-01,\n",
      "          2.1606e-01,  8.2558e-01, -2.0366e-01, -7.5206e-01, -1.1916e-01,\n",
      "         -3.3562e-01, -1.5486e-01,  2.8654e-01, -2.8491e-01, -3.3805e-01,\n",
      "         -4.6330e-01, -4.8639e-01, -6.0603e-01,  7.5654e-02,  1.3147e+00,\n",
      "         -7.2745e-01, -4.0140e-01,  1.6559e-01, -2.1778e-01, -4.8973e-02,\n",
      "         -1.5176e-01,  3.9247e-01,  4.0493e-02,  2.5240e-01,  9.8921e-02,\n",
      "         -2.8206e-01, -4.5346e-01,  8.9546e-02, -1.0008e-01, -2.5147e-01,\n",
      "         -3.7542e-01, -1.5013e-01, -4.4049e-01, -5.2798e-01, -3.9284e-01,\n",
      "         -5.5502e-01, -5.6176e-01,  4.5016e-01,  6.6317e-01,  1.9641e-01,\n",
      "         -8.7128e-02,  2.0580e-01,  2.4340e-01, -1.6681e-01,  7.1293e-02,\n",
      "         -2.7083e-01, -1.2668e-01, -2.2112e-01,  4.7699e-01, -1.8108e-01,\n",
      "          6.2314e-01, -1.3760e-02, -1.2871e-01,  4.2502e-01, -1.2754e-01,\n",
      "         -8.5973e-02,  6.9321e-02, -8.5293e-01, -6.6570e-01,  7.3446e-02,\n",
      "         -4.1798e-01,  2.0650e-01,  4.1419e-01,  1.8778e-01,  2.3166e-02,\n",
      "          3.4826e-01,  5.0634e-01,  2.2473e-02, -3.2010e-01,  1.7244e-01,\n",
      "         -6.3321e-01, -3.4765e-02,  3.4278e-01, -1.3378e+00,  3.3178e-01,\n",
      "         -3.0396e-01, -1.8240e-01, -2.2205e-01, -2.7592e-01, -5.0671e-01,\n",
      "          1.0677e-02,  5.8438e-01,  7.9217e-01, -1.5844e-01,  1.7018e-01,\n",
      "          3.9600e-01, -7.9226e-03,  1.6647e-01, -1.6082e-01,  3.5374e-01,\n",
      "          2.2075e-01,  2.5070e-01,  3.6525e-01, -5.9018e-01,  6.2041e-03,\n",
      "          1.8279e-02, -8.0088e-02, -1.4627e-01, -1.9641e-01, -7.8911e-02,\n",
      "         -1.5711e-01, -1.4614e-02,  6.0073e-01,  5.3795e-01,  1.5774e-02,\n",
      "         -6.9795e-01, -3.6461e-01, -1.9467e-01, -1.0206e+00, -4.5848e-02,\n",
      "         -6.4804e-01, -2.8571e-01,  2.5250e-01,  1.8444e-01,  2.6934e-01,\n",
      "         -4.0029e-01,  7.0506e-03,  7.4742e-01,  6.6999e-01, -4.3562e-01,\n",
      "         -1.4035e-01, -6.7726e-01, -6.6741e-01,  1.9602e-01,  1.8807e-02,\n",
      "          4.8666e-01, -2.1115e-01, -2.0445e-01,  1.0416e+00, -7.8200e-01,\n",
      "         -4.7199e-01, -1.8492e-01, -5.6026e-01, -6.0901e-01, -9.3033e-01,\n",
      "          2.3673e-01,  2.9754e-01,  2.6468e-01, -1.1192e+00,  8.1581e-02,\n",
      "         -5.6938e-01, -3.4759e-01,  1.9614e-01, -7.2282e-01, -7.3471e-01,\n",
      "         -5.0561e-01, -1.0223e+00, -2.9108e-01, -5.9854e-01, -1.4229e-01,\n",
      "         -2.7401e-01, -6.1035e-01, -4.7884e-02, -3.2155e-01,  4.3654e-01,\n",
      "         -1.9838e-01, -5.3988e-02, -2.6979e-01, -8.9624e-02, -6.2506e-01,\n",
      "          1.1542e+00,  6.1223e-02,  7.2652e-01, -4.0541e-01,  1.0979e-01,\n",
      "          1.7359e-01, -1.3109e+00,  2.4553e-01, -8.9277e-01,  9.0673e-02,\n",
      "          2.3769e-01, -6.4128e-01,  4.7676e-01, -3.3163e-01, -1.0115e+00,\n",
      "          3.5212e-01, -8.0840e-01,  1.3466e-01,  7.7935e-02,  3.6161e-01,\n",
      "         -4.0338e-01, -4.2909e-01, -2.8330e-01, -6.2297e-01, -2.9815e-02,\n",
      "         -4.1358e-01,  6.1541e-01,  5.6180e-01,  1.1528e-01, -6.2102e-02,\n",
      "          2.7615e-01, -5.3368e-01, -8.6152e-02, -4.4085e-01,  8.7784e-01,\n",
      "         -5.1701e-01, -4.5535e-01, -3.7511e-01,  8.0284e-02,  1.1892e+00,\n",
      "         -1.5782e-01, -1.7400e-01,  1.2496e-01, -1.9838e-01, -3.6099e-01,\n",
      "         -3.0070e-01, -6.0914e-01, -7.9083e-02, -6.7315e-02, -3.7093e-01,\n",
      "         -6.7242e-01, -1.1398e-01, -3.8061e-01, -1.2380e-02,  2.2480e-01,\n",
      "         -1.7509e+00,  9.0184e-02,  2.0881e-03, -1.5649e-01,  4.8585e-01,\n",
      "          7.3094e-03, -1.7017e-01,  3.7798e-01, -1.8711e-01, -6.9332e-01,\n",
      "          1.5565e-01, -7.2735e-01, -2.7711e-01,  2.5302e-01, -6.9795e-01,\n",
      "          1.6739e+00,  1.4785e-01,  3.5852e-01,  8.6444e-02,  8.1378e-03,\n",
      "         -7.0509e-01,  2.6969e-01,  6.9479e-01, -8.4471e-01, -1.5853e-01,\n",
      "         -3.5194e-01,  7.3336e-02,  3.5219e-01, -1.2464e+00,  2.7804e-01,\n",
      "         -5.5141e-02, -3.0245e-01,  5.2282e-02, -2.1834e-01, -1.2545e+00,\n",
      "         -4.6060e-02, -9.7412e-02, -1.1815e-01]])\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddings = last_hidden_states.mean(1)\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c503c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 424, 768])\n",
      "tensor([[ 0.3131,  0.1495, -0.2805,  ..., -1.0458,  0.2955,  0.2060],\n",
      "        [-0.6040, -0.3774,  0.7601,  ..., -0.2936,  0.6367, -0.2034],\n",
      "        [-0.9364, -0.4989,  1.5890,  ..., -0.1287,  0.1404,  0.0332],\n",
      "        ...,\n",
      "        [-0.7169,  0.1413,  1.1147,  ..., -1.0553,  0.1065, -0.3283],\n",
      "        [ 0.0473, -0.1965,  0.2112,  ..., -1.1475, -0.5695,  0.0044],\n",
      "        [ 0.2168,  0.0173, -0.6936,  ..., -0.9997, -0.1208,  0.1229]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "# Transform input tokens \n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Model apply\n",
    "outputs = model(**inputs)\n",
    "\n",
    "outputs.last_hidden_state.shape\n",
    "outputs.pooler_output.shape\n",
    "outputs.keys()\n",
    "print(outputs.last_hidden_state.shape)\n",
    "print(outputs.last_hidden_state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7dcf3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/ainize/kobart-news\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "#  Load Model and Tokenize\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"ainize/kobart-news\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"ainize/kobart-news\")\n",
    "\n",
    "def summarize(model, tokenizer, input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    last_hidden_states = model(input_ids)[0]\n",
    "    print('embeddings: ', last_hidden_states)\n",
    "    print(last_hidden_states.shape)\n",
    "    print('input ids: ', input_ids, len(input_ids[0]))\n",
    "    # Generate Summary Text Ids\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        length_penalty=2.0,\n",
    "        max_length=142,\n",
    "        min_length=56,\n",
    "        num_beams=4,\n",
    "    )\n",
    "    print('output ids: ', output_ids[0], len(output_ids[0]))\n",
    "    # Decoding Text\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b2b45e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings:  tensor([[[ 31.6866,   9.3366,  11.2337,  ...,   4.5137,   7.3841,   7.1725],\n",
      "         [ -2.3933,   2.1697,  -7.8838,  ...,  -7.2838,  -9.8763,  -5.3370],\n",
      "         [ -4.9029,  -0.6490,  -9.7926,  ...,  -8.6843, -15.5855,  -8.3866],\n",
      "         ...,\n",
      "         [ -1.7835,   2.1035,  -2.8559,  ...,  -4.8765,  -3.2337,  -3.6935],\n",
      "         [ -2.4649,   5.5784,  -1.4122,  ...,  -4.5981,  -0.9677,  -4.7450],\n",
      "         [ -4.5541,  16.5802,  -7.4921,  ...,  -5.8620,  -9.5876, -11.4848]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 365, 30000])\n",
      "input ids:  tensor([[    0, 23436, 25623, 27091, 21067, 23895, 16442, 14453, 15342,  9866,\n",
      "         14770, 14134, 15665, 14896, 16553, 14082, 14173, 19743, 14130, 15342,\n",
      "         13679, 15821, 22164, 14451, 23895, 15107, 14867, 17047, 14188,  8995,\n",
      "         22036, 14109, 15161, 22269, 14494, 14770, 22537, 13679, 23895, 14408,\n",
      "         18801, 16162,   239, 23625, 14030, 19668, 17249, 22540,   243, 18453,\n",
      "         22540, 14030, 14060, 10773, 14051, 15541, 14061, 18010, 14408, 16162,\n",
      "         15019, 20646,  1700, 17867,  9770, 19443,   251, 18604, 15359, 20412,\n",
      "         18078,   252,   255, 14079, 15624, 15665, 14781, 16826, 14253, 14130,\n",
      "         17328,   243, 16937, 17883, 14915, 28018, 14030, 14408, 18801, 16162,\n",
      "         12034, 14377, 14605, 25572, 22832, 20412, 16248,   250, 21395, 14810,\n",
      "         19115, 16176, 16077, 20646, 16248,   256, 21395, 11936, 15171, 19424,\n",
      "         20750, 15665, 15615, 14311, 15027,   239, 16964,   254, 15990, 18056,\n",
      "         14245,   239, 16964,   253, 16157, 18056, 15122,   239, 16964,   251,\n",
      "         16157, 18056, 14802,   239, 16964,   250, 15318, 18056, 16937,   239,\n",
      "         16964,   250, 15899, 18056, 16061,   239, 16964,   248, 18056, 14371,\n",
      "           239,   244, 15874, 15478, 18056, 15058,   239, 16280,   255, 15318,\n",
      "         16705, 24991, 15665, 15615, 14915, 26995, 18670, 16082, 15615, 15857,\n",
      "         12024, 20412, 14408, 18801, 16162, 12005, 16248, 20374, 14079, 15624,\n",
      "         20646, 16248,   254,   251, 24542, 15171, 15431, 20750, 15665, 17406,\n",
      "         15228,   239, 16964,   252, 15275, 18056, 16621,   239, 16964,   251,\n",
      "         15275, 18056, 17484,   239,   244, 14745, 16157, 18056, 16329,   239,\n",
      "           244, 14745, 16157, 18056, 16309,   239,   244, 14745, 15275, 18056,\n",
      "         17159,   239, 16280,   254, 18056, 18359,   239, 16280,   252, 15625,\n",
      "         16705, 18113, 15665, 14896, 18216, 14130, 27924, 13146, 23895, 15107,\n",
      "         17562, 17178, 28138, 21586, 14371, 12024, 22837,  9908, 20720, 15047,\n",
      "         18012, 14066, 18652, 20049, 14382, 16464, 15342, 13679, 15821, 18094,\n",
      "         15461, 14511, 14051, 12333,  9908, 15266, 18216,  9102, 21628,   243,\n",
      "         16554, 14934, 14671, 14624, 15342, 13679, 15821, 15390,  9866, 14777,\n",
      "         13607, 14061, 21434, 15532,  9866, 14481, 11779, 14510, 14050, 11471,\n",
      "         12147, 14171, 20412, 14770, 22537, 13679, 23895, 14061, 11471, 18178,\n",
      "         14460, 15318, 16445, 21349, 20646, 14460, 20597, 15171, 14894, 20742,\n",
      "         19971, 14777, 17351, 20292, 14329, 24952, 21638, 12922, 23246, 16851,\n",
      "         17692, 10443, 15929, 16246, 11224, 15471, 14505, 16377, 22556, 26498,\n",
      "         25267,   243, 22162, 27236, 15337, 25267, 14624, 14442, 29019, 29180,\n",
      "         14174, 15068, 15587, 14097, 12710, 14510, 14050, 29698, 14331,   243,\n",
      "         17072, 11786, 15176, 14066,  9190, 26061, 13714, 28598, 14291, 21711,\n",
      "         14173, 29106, 15411, 20029,     1]]) 365\n",
      "output ids:  tensor([    2,     0, 12126, 10773, 14134, 14371, 14836, 21067, 16442, 12258,\n",
      "        15342, 12034, 14770, 14134, 15665, 14896, 16412, 14125, 14770, 22537,\n",
      "        13679, 23895, 14408, 18801, 16162, 12034, 20412, 18078,   252,   255,\n",
      "        14079, 15624, 15665, 17406, 27924, 13146, 23895, 15107, 17562, 17178,\n",
      "        15023, 14331,   243, 17072, 11786, 15176, 14066,  9190, 26061, 13714,\n",
      "        20820, 14291, 21711, 14173, 14805, 14117, 14253, 14130,     1]) 59\n",
      "전반적인 경기 침체로 건물주 수익이 전국적인 감소세를 보이면서 전국 중대형 상가 순영업소득이 3분기 2만5800원으로 감소했으며 조현택 상가정보연구소 연구원은 앞으로 지역, 콘텐츠에 따른 상권 양극화 현상이 심화될 것으로 보인다고 밝혔다.\n"
     ]
    }
   ],
   "source": [
    "# Encode Input Text\n",
    "input_text = '국내 전반적인 경기침체로 상가 건물주의 수익도 전국적인 감소세를 보이고 있는 것으로 나타났다. 수익형 부동산 연구개발기업 상가정보연구소는 한국감정원 통계를 분석한 결과 전국 중대형 상가 순영업소득(부동산에서 발생하는 임대수입, 기타수입에서 제반 경비를 공제한 순소득)이 1분기 ㎡당 3만4200원에서 3분기 2만5800원으로 감소했다고 17일 밝혔다. 수도권, 세종시, 지방광역시에서 순영업소득이 가장 많이 감소한 지역은 3분기 1만3100원을 기록한 울산으로, 1분기 1만9100원 대비 31.4% 감소했다. 이어 대구(-27.7%), 서울(-26.9%), 광주(-24.9%), 부산(-23.5%), 세종(-23.4%), 대전(-21%), 경기(-19.2%), 인천(-18.5%) 순으로 감소했다. 지방 도시의 경우도 비슷했다. 경남의 3분기 순영업소득은 1만2800원으로 1분기 1만7400원 대비 26.4% 감소했으며 제주(-25.1%), 경북(-24.1%), 충남(-20.9%), 강원(-20.9%), 전남(-20.1%), 전북(-17%), 충북(-15.3%) 등도 감소세를 보였다. 조현택 상가정보연구소 연구원은 \"올해 내수 경기의 침체된 분위기가 유지되며 상가, 오피스 등을 비롯한 수익형 부동산 시장의 분위기도 경직된 모습을 보였고 오피스텔, 지식산업센터 등의 수익형 부동산 공급도 증가해 공실의 위험도 늘었다\"며 \"실제 올 3분기 전국 중대형 상가 공실률은 11.5%를 기록하며 1분기 11.3% 대비 0.2% 포인트 증가했다\"고 말했다. 그는 \"최근 소셜커머스(SNS를 통한 전자상거래), 음식 배달 중개 애플리케이션, 중고 물품 거래 애플리케이션 등의 사용 증가로 오프라인 매장에 영향을 미쳤다\"며 \"향후 지역, 콘텐츠에 따른 상권 양극화 현상은 심화될 것으로 보인다\"고 덧붙였다.'\n",
    "decoded_text = summarize(model, tokenizer, input_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c9cae0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국국의 안경 및 콘택트 렌즈, 선글라스 등 판매업체 와비 파커의 2분기 주당순손실은 0.01달러로 월가의 예상치 0.02달러에 하회했으며 이 기간 매출은 1억4,960만달러로 시장 예상치에는 약간 상회한 것으로 나타났고 올해 매출 전망도 대폭 하향 조정한다고 밝혔다.\n"
     ]
    }
   ],
   "source": [
    "# Encode Input Text\n",
    "input_text = \"미국의 안경 및 콘택트 렌즈, 선글라스 등 판매업체 와비 파커의 2분기 주당순손실은 0.01달러로 월가의 예상치 0.02달러에 하회한 것으로 나타났다. 11일(현지시간) 와비 파커에 따르면 이 기간 매출은 1억4,960만달러로 시장 예상치 1억4,950만달러에는 약간 상회했다. 스티브 밀러 최고재무책임자(CFO)는 우리는 대부분의 기업들과 마찬가지로 불확실한 거시경제 환경에 직면하고 있다며 올들어 매출 부진이 이어져 손실이 확대되고 있고, 이에 올해 매출 전망도 대폭 하향 조정한다고 밝혔다\"\n",
    "decoded_text = summarize(model, tokenizer, input_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a665ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 156, 30000])\n",
      "input ids:  tensor([[    0,   230,   290, 17303, 17515,   292, 14099, 13672, 13699, 14680,\n",
      "         15146, 27070, 17047, 16670, 14053,  9190, 11224, 12074, 16817, 20050,\n",
      "         19571, 18556, 14493, 17817, 11900, 16000, 20205, 13607, 14066, 15068,\n",
      "         14184, 19618, 14173, 16053, 14781, 14278,   230, 25845, 14137, 18837,\n",
      "         11729, 14087, 12943, 12926, 10476, 14585, 17547, 18556, 14298, 14141,\n",
      "         15459, 14070, 27070, 16755, 17817, 25917, 15453, 15615, 14383, 28319,\n",
      "         15322, 25917, 16342, 13590, 14237,   230, 12926, 15470, 24813, 14832,\n",
      "         14437, 15240, 17449, 17848, 19479, 17918, 18865, 15249, 11323, 19822,\n",
      "         16167, 26055, 14395, 14052,   230, 15546, 15178, 27744, 18556, 14298,\n",
      "         17412, 10795, 11331, 14048, 16246, 11224, 15471, 24563, 15385, 14126,\n",
      "         11973, 14049, 15454, 14024,   230, 12926, 15470, 14979, 14053, 14785,\n",
      "         14287, 14115, 12984, 15740, 25162, 28080, 16030,   254, 18056, 14361,\n",
      "          9092, 18645, 23827, 25162, 14766,   248, 16030,   256, 18056, 15976,\n",
      "          9092, 17605, 21132, 12034, 18636,  9611, 22236, 21348, 22458, 15275,\n",
      "           254,   236, 15882, 14516, 14053, 17415, 19404, 13590, 26001, 11786,\n",
      "         13030, 13358, 15113, 22280, 19796,     1]]) 156\n",
      "output ids:  tensor([    2,     0, 15274, 15471, 17047, 16670, 14053,  9190, 11224, 12074,\n",
      "        16817, 20050, 19571, 16039, 14087, 12943, 12926, 10476, 14585, 17547,\n",
      "        18556, 14493, 14437, 15240, 17449, 17848, 19479, 17918, 18865, 15249,\n",
      "        11323, 19822, 16167, 26055, 14395, 15479, 17412, 10795, 11331, 14048,\n",
      "        16246, 11224, 15471, 24563, 15385, 14126, 11973, 14049, 15454, 10338,\n",
      "         1700, 14053,  9190, 11224, 12074, 16817, 20050, 19571, 18556, 14493,\n",
      "        17817, 11900, 16000, 20205, 13607, 14066, 15068, 14184, 19618, 14173,\n",
      "        16053, 15568, 14117, 14253, 14297,  1700, 14383, 28319, 15322, 25917,\n",
      "        16342, 13590, 14237, 12926, 15470, 24813, 14832, 14437, 15240, 17449,\n",
      "        17848, 19479, 17918, 18865, 15249, 11323, 19822, 16167, 26055, 14395,\n",
      "        16247,     1]) 102\n",
      "\n",
      "[서울=뉴시스] 최현호 기자 = 한국거래소는 22일 주권상장 예비심사 결과, 컬리가 상장요건을 충족해 상장에 적격한 것으로 확정했다고 밝혔다.\n",
      "\n",
      "온라인 장보기앱 마켓컬리 운영사인 컬리는 지난 3월 말 한국거래소에 상장 심사를 신청했다. 약 5개월 만에 심사를 통과한 것이다.\n",
      "\n",
      "컬리의 별도 기준 지난해 매출액은 1조 5580억원, 영업손실은 2139억원이다.\n",
      "\n",
      "2014년 설립된 컬리는 새벽배송 등 전자상거래 소매업을 영위하는 회사다.\n",
      "\n",
      "컬리의 주요 주주는 미국 세콰이어캐피탈(12.87%), 중국계 힐하우스캐피탈(11.89%), 러시아계 디지털스카이테크놀로지글로벌(10.17%)과 올해 주주로 합류한 앵커에쿼티파트너스 등이다.\n",
      "\n",
      "한국거래소는 22일 주권상장 예비심사 결과, 온라인 마켓컬리 운영사인 컬리가 지난해 매출액은 1조 5580억원, 영업손실은 2139억원이며 새벽배송 등 전자상거래 소매업을 영위하는 회사로  주권상장 예비심사 결과, 컬리가 상장요건을 충족해 상장에 적격한 것으로 확정되었다고 밝혔으며  약 5개월 만에 심사를 통과한 것이다.\n",
      "컬리의 별도 기준 지난해 매출액은 1조 5580억원, 영업손실은 2139억원이다.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "[서울=뉴시스] 최현호 기자 = 한국거래소는 22일 주권상장 예비심사 결과, 컬리가 상장요건을 충족해 상장에 적격한 것으로 확정했다고 밝혔다.\n",
    "\n",
    "온라인 장보기앱 마켓컬리 운영사인 컬리는 지난 3월 말 한국거래소에 상장 심사를 신청했다. 약 5개월 만에 심사를 통과한 것이다.\n",
    "\n",
    "컬리의 별도 기준 지난해 매출액은 1조 5580억원, 영업손실은 2139억원이다.\n",
    "\n",
    "2014년 설립된 컬리는 새벽배송 등 전자상거래 소매업을 영위하는 회사다.\n",
    "\n",
    "컬리의 주요 주주는 미국 세콰이어캐피탈(12.87%), 중국계 힐하우스캐피탈(11.89%), 러시아계 디지털스카이테크놀로지글로벌(10.17%)과 올해 주주로 합류한 앵커에쿼티파트너스 등이다.\n",
    "\"\"\"\n",
    "decoded_text = summarize(model, tokenizer, input_text)\n",
    "print(input_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439706d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f77aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c2877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
